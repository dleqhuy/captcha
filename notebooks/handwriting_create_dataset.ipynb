{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4933d416-36e6-40f3-aa43-f46e78e624f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://git.io/J0fjL -O IAM_Words.zip\n",
    "!unzip -qq IAM_Words.zip\n",
    "!\n",
    "!mkdir data\n",
    "!mkdir data/words\n",
    "!tar -xf IAM_Words/words.tgz -C data/words\n",
    "!mv IAM_Words/words.txt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1432ffef-d4fd-49a1-ae19-750d9000626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f999019a-7ac0-4b90-b55c-a9c9b074d6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/studio-lab-user/data\"\n",
    "words_list = []\n",
    "\n",
    "words = open(f\"{base_path}/words.txt\", \"r\").readlines()\n",
    "for line in words:\n",
    "    if line[0] == \"#\":\n",
    "        continue\n",
    "    if line.split(\" \")[1] != \"err\":  # We don't need to deal with errored entries.\n",
    "        words_list.append(line)\n",
    "\n",
    "len(words_list)\n",
    "\n",
    "np.random.shuffle(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7f1c897-7b5e-4336-bc64-664615a7344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_image_path = os.path.join(base_path, \"words\")\n",
    "\n",
    "\n",
    "def get_image_paths_and_labels(samples):\n",
    "    paths = []\n",
    "    corrected_samples = []\n",
    "    for (i, file_line) in enumerate(samples):\n",
    "        line_split = file_line.strip()\n",
    "        line_split = line_split.split(\" \")\n",
    "\n",
    "        # Each line split will have this format for the corresponding image:\n",
    "        # part1/part1-part2/part1-part2-part3.png\n",
    "        image_name = line_split[0]\n",
    "        partI = image_name.split(\"-\")[0]\n",
    "        partII = image_name.split(\"-\")[1]\n",
    "        img_path = os.path.join(\n",
    "            base_image_path, partI, partI + \"-\" + partII, image_name + \".png\"\n",
    "        )\n",
    "        if os.path.getsize(img_path):\n",
    "            paths.append(img_path)\n",
    "            corrected_samples.append(file_line.split(\"\\n\")[0])\n",
    "\n",
    "    return paths, corrected_samples\n",
    "\n",
    "\n",
    "img_paths, labels = get_image_paths_and_labels(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "241c43a9-1796-446a-943d-7f753cf9a17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length:  21\n",
      "Vocab size:  78\n"
     ]
    }
   ],
   "source": [
    "# Find maximum length and the size of the vocabulary in the training data.\n",
    "labels_cleaned = []\n",
    "characters = set()\n",
    "max_len = 0\n",
    "\n",
    "for label in labels:\n",
    "    label = label.split(\" \")[-1].strip()\n",
    "    for char in label:\n",
    "        characters.add(char)\n",
    "\n",
    "    max_len = max(max_len, len(label))\n",
    "    labels_cleaned.append(label)\n",
    "\n",
    "characters = sorted(list(characters))\n",
    "\n",
    "print(\"Maximum length: \", max_len)\n",
    "print(\"Vocab size: \", len(characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce52050d-a2ce-4930-949c-5178b91dbb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame( \n",
    "    {'file_path': img_paths,\n",
    "     'label': labels_cleaned\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a967b71d-c099-430a-bc23-aa1f644ccf9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/studio-lab-user/data/words/r06/r06-076/r...</td>\n",
       "      <td>or</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/studio-lab-user/data/words/n01/n01-004/n...</td>\n",
       "      <td>unable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/studio-lab-user/data/words/g06/g06-011f/...</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/studio-lab-user/data/words/f04/f04-011/f...</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/studio-lab-user/data/words/e04/e04-103/e...</td>\n",
       "      <td>plank</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path   label\n",
       "0  /home/studio-lab-user/data/words/r06/r06-076/r...      or\n",
       "1  /home/studio-lab-user/data/words/n01/n01-004/n...  unable\n",
       "2  /home/studio-lab-user/data/words/g06/g06-011f/...      of\n",
       "3  /home/studio-lab-user/data/words/f04/f04-011/f...     was\n",
       "4  /home/studio-lab-user/data/words/e04/e04-103/e...   plank"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e67a023a-1add-461e-8d48-0fb40b3f8b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/home/studio-lab-user/handwriting/handwriting.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr_crnn:Python",
   "language": "python",
   "name": "conda-env-ocr_crnn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
